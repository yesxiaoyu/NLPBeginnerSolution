{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Change encoding from BIO to BIOES')\n",
    "# parser.add_argument('input', type=str, default='./data/',help='The path to the original file with BIO encoding')\n",
    "# parser.add_argument('output', type=str, default='./out_data/', help='The name of your BIOES encoded file')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# input_file = args.input\n",
    "# output_file = args.output\n",
    "\n",
    "\n",
    "def read_file(input_file):\n",
    "    with open(input_file, 'r', encoding=\"utf8\") as f:\n",
    "        return f.read().split('\\n')[:-1]\n",
    "\n",
    "\n",
    "def write_line(new_label, prev_label, line_content, output_file):\n",
    "    new_iob = new_label + prev_label\n",
    "    line_content[-1] = new_iob\n",
    "    current_line = ' '.join(line_content)\n",
    "    output_file.write(current_line + '\\n')\n",
    "\n",
    "\n",
    "def not_same_tag(tag1, tag2):\n",
    "    return tag1.split(\"-\")[-1] != tag2.split(\"-\")[-1]\n",
    "\n",
    "\n",
    "def convert(input_file, output_path):\n",
    "    output_file = open(output_path, 'w', encoding=\"utf8\")\n",
    "\n",
    "    for i in range(len(input_file)):\n",
    "\n",
    "        try:\n",
    "            current_line = input_file[i]\n",
    "\n",
    "            if '-DOCSTART-' in current_line:\n",
    "                output_file.write(current_line + \"\\n\")\n",
    "            elif len(current_line.strip()) == 0:\n",
    "                output_file.write(current_line.strip() + \"\\n\")\n",
    "                output_file.flush()\n",
    "            else:\n",
    "                output_file.flush()\n",
    "                if i == 280:\n",
    "                    print(current_line)\n",
    "                prev_iob = \"\"\n",
    "                next_iob = \"\"\n",
    "                prev_line = None\n",
    "                next_line = None\n",
    "\n",
    "                try:\n",
    "                    prev_line = input_file[i - 1]\n",
    "                    next_line = input_file[i + 1]\n",
    "\n",
    "                    if len(prev_line.strip()) > 0:\n",
    "                        prev_line_content = prev_line.split()\n",
    "                        prev_iob = prev_line_content[-1]\n",
    "\n",
    "                    if len(next_line.strip()) > 0:\n",
    "                        next_line_content = next_line.split()\n",
    "                        next_iob = next_line_content[-1]\n",
    "\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "                current_line_content = current_line.strip().split()\n",
    "                current_iob = current_line_content[-1]\n",
    "\n",
    "                # Outside entities\n",
    "                if current_iob == 'O':\n",
    "                    output_file.write(current_line + \"\\n\")\n",
    "\n",
    "                # Unit length entities\n",
    "                elif current_iob.startswith(\"B-\") and \\\n",
    "                        (next_iob == 'O' or len(next_line.strip()) == 0 or next_iob.startswith(\"B-\")):\n",
    "                    write_line('S-', current_iob[2:], current_line_content, output_file)\n",
    "\n",
    "                # First element of chunk\n",
    "                elif current_iob.startswith(\"B-\") and \\\n",
    "                        (not not_same_tag(current_iob, next_iob) and next_iob.startswith(\"I-\")):\n",
    "                    write_line('B-', current_iob[2:], current_line_content, output_file)\n",
    "\n",
    "                # Last element of chunk\n",
    "                elif current_iob.startswith(\"I-\") and \\\n",
    "                        (next_iob == 'O' or len(next_line.strip()) == 0 or next_iob.startswith(\"B-\")):\n",
    "                    write_line('E-', current_iob[2:], current_line_content, output_file)\n",
    "\n",
    "                # Inside a chunk\n",
    "                elif current_iob.startswith(\"I-\") and \\\n",
    "                        next_iob.startswith(\"I-\"):\n",
    "                    write_line('I-', current_iob[2:], current_line_content, output_file)\n",
    "\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CRF(nn.Module):\n",
    "    def __init__(self, label_size):\n",
    "        '''label_size = real size + 2, included START and END '''\n",
    "        super(CRF, self).__init__()\n",
    "\n",
    "        self.label_size = label_size\n",
    "        self.start = self.label_size - 2\n",
    "        self.end = self.label_size - 1\n",
    "        transition = torch.randn(self.label_size, self.label_size)\n",
    "        self.transition = nn.Parameter(transition)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.uniform_(self.transition.data, -0.1, 0.1)\n",
    "        self.transition.data[:, self.end] = -1000.0\n",
    "        self.transition.data[self.start, :] = -1000.0\n",
    "\n",
    "    def pad_logits(self, logits):\n",
    "        batch_size, seq_len, label_num = logits.size()\n",
    "        pads = logits.new_full((batch_size, seq_len, 2), -1000.0,\n",
    "                               requires_grad=False)\n",
    "        logits = torch.cat([logits, pads], dim=2)\n",
    "        return logits\n",
    "\n",
    "    def calc_binary_score(self, labels, predict_mask):\n",
    "        '''\n",
    "        Gold transition score\n",
    "        :param labels: [batch_size, seq_len] LongTensor\n",
    "        :param predict_mask: [batch_size, seq_len] LongTensor\n",
    "        :return: [batch_size] FloatTensor\n",
    "        '''\n",
    "        batch_size, seq_len = labels.size()\n",
    "\n",
    "        labels_ext = labels.new_empty((batch_size, seq_len + 2))\n",
    "        labels_ext[:, 0] = self.start\n",
    "        labels_ext[:, 1:-1] = labels\n",
    "        labels_ext[:, -1] = self.end\n",
    "        pad = predict_mask.new_ones([batch_size, 1], requires_grad=False)\n",
    "        pad_stop = labels.new_full([batch_size, 1], self.end, requires_grad=False)\n",
    "        mask = torch.cat([pad, predict_mask, pad], dim=-1).long()\n",
    "        labels = (1 - mask) * pad_stop + mask * labels_ext\n",
    "\n",
    "        trn = self.transition\n",
    "        trn_exp = trn.unsqueeze(0).expand(batch_size, *trn.size())\n",
    "        lbl_r = labels[:, 1:]\n",
    "        lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), trn.size(0))\n",
    "        trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n",
    "\n",
    "        lbl_lexp = labels[:, :-1].unsqueeze(-1)\n",
    "        trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n",
    "        trn_scr = trn_scr.squeeze(-1)\n",
    "\n",
    "        mask = torch.cat([pad, predict_mask], dim=-1).float()\n",
    "        trn_scr = trn_scr * mask\n",
    "        score = trn_scr\n",
    "\n",
    "        return score\n",
    "\n",
    "    def calc_unary_score(self, logits, labels, predict_mask):\n",
    "        '''\n",
    "        Gold logits score\n",
    "        :param logits: [batch_size, seq_len, n_labels] FloatTensor\n",
    "        :param labels: [batch_size, seq_len] LongTensor\n",
    "        :param predict_mask: [batch_size, seq_len] LongTensor\n",
    "        :return: [batch_size] FloatTensor\n",
    "        '''\n",
    "        labels_exp = labels.unsqueeze(-1)\n",
    "        scores = torch.gather(logits, 2, labels_exp).squeeze(-1)\n",
    "        scores = scores * predict_mask.float()\n",
    "        return scores\n",
    "\n",
    "    def calc_gold_score(self, logits, labels, predict_mask):\n",
    "        '''\n",
    "        Total score of gold sequence.\n",
    "        :param logits: [batch_size, seq_len, n_labels] FloatTensor\n",
    "        :param labels: [batch_size, seq_len] LongTensor\n",
    "        :param predict_mask: [batch_size, seq_len] LongTensor\n",
    "        :return: [batch_size] FloatTensor\n",
    "        '''\n",
    "        unary_score = self.calc_unary_score(logits, labels, predict_mask).sum(\n",
    "            1).squeeze(-1)\n",
    "        # print(unary_score)\n",
    "        binary_score = self.calc_binary_score(labels, predict_mask).sum(1).squeeze(-1)\n",
    "        # print(binary_score)\n",
    "        return unary_score + binary_score\n",
    "\n",
    "    def calc_norm_score(self, logits, predict_mask):\n",
    "        '''\n",
    "        Total score of all sequences.\n",
    "        :param logits: [batch_size, seq_len, n_labels] FloatTensor\n",
    "        :param predict_mask: [batch_size, seq_len] LongTensor\n",
    "        :return: [batch_size] FloatTensor\n",
    "        '''\n",
    "        batch_size, seq_len, feat_dim = logits.size()\n",
    "\n",
    "        alpha = logits.new_full((batch_size, self.label_size), -100.0)\n",
    "        alpha[:, self.start] = 0\n",
    "\n",
    "        predict_mask_ = predict_mask.clone()  # (batch_size, max_seq)\n",
    "\n",
    "        logits_t = logits.transpose(1, 0)  # (max_seq, batch_size, num_labels + 2)\n",
    "        predict_mask_ = predict_mask_.transpose(1, 0)  # (max_seq, batch_size)\n",
    "        for word_mask_, logit in zip(predict_mask_, logits_t):\n",
    "            logit_exp = logit.unsqueeze(-1).expand(batch_size,\n",
    "                                                   *self.transition.size())\n",
    "            alpha_exp = alpha.unsqueeze(1).expand(batch_size,\n",
    "                                                  *self.transition.size())\n",
    "            trans_exp = self.transition.unsqueeze(0).expand_as(alpha_exp)\n",
    "            mat = logit_exp + alpha_exp + trans_exp\n",
    "            alpha_nxt = log_sum_exp(mat, 2).squeeze(-1)\n",
    "\n",
    "            mask = word_mask_.float().unsqueeze(-1).expand_as(alpha)  # (batch_size, num_labels+2)\n",
    "            alpha = mask * alpha_nxt + (1 - mask) * alpha\n",
    "\n",
    "        alpha = alpha + self.transition[self.end].unsqueeze(0).expand_as(alpha)\n",
    "        norm = log_sum_exp(alpha, 1).squeeze(-1)\n",
    "\n",
    "        return norm\n",
    "\n",
    "    def viterbi_decode(self, logits, predict_mask):\n",
    "        \"\"\"\n",
    "        :param logits: [batch_size, seq_len, n_labels] FloatTensor\n",
    "        :param predict_mask: [batch_size, seq_len] LongTensor\n",
    "        :return scores: [batch_size] FloatTensor\n",
    "        :return paths: [batch_size, seq_len] LongTensor\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_labels = logits.size()\n",
    "        vit = logits.new_full((batch_size, self.label_size), -100.0)\n",
    "        vit[:, self.start] = 0\n",
    "        predict_mask_ = predict_mask.clone()  # (batch_size, max_seq)\n",
    "        predict_mask_ = predict_mask_.transpose(1, 0)  # (max_seq, batch_size)\n",
    "        logits_t = logits.transpose(1, 0)\n",
    "        pointers = []\n",
    "        for ix, logit in enumerate(logits_t):\n",
    "            vit_exp = vit.unsqueeze(1).expand(batch_size, n_labels, n_labels)\n",
    "            trn_exp = self.transition.unsqueeze(0).expand_as(vit_exp)\n",
    "            vit_trn_sum = vit_exp + trn_exp\n",
    "            vt_max, vt_argmax = vit_trn_sum.max(2)\n",
    "\n",
    "            vt_max = vt_max.squeeze(-1)\n",
    "            vit_nxt = vt_max + logit\n",
    "            pointers.append(vt_argmax.squeeze(-1).unsqueeze(0))\n",
    "\n",
    "            mask = predict_mask_[ix].float().unsqueeze(-1).expand_as(vit_nxt)\n",
    "            vit = mask * vit_nxt + (1 - mask) * vit\n",
    "\n",
    "            mask = (predict_mask_[ix:].sum(0) == 1).float().unsqueeze(-1).expand_as(vit_nxt)\n",
    "            vit += mask * self.transition[self.end].unsqueeze(\n",
    "                0).expand_as(vit_nxt)\n",
    "\n",
    "        pointers = torch.cat(pointers)\n",
    "        scores, idx = vit.max(1)\n",
    "        paths = [idx.unsqueeze(1)]\n",
    "        for argmax in reversed(pointers):\n",
    "            idx_exp = idx.unsqueeze(-1)\n",
    "            idx = torch.gather(argmax, 1, idx_exp)\n",
    "            idx = idx.squeeze(-1)\n",
    "\n",
    "            paths.insert(0, idx.unsqueeze(1))\n",
    "\n",
    "        paths = torch.cat(paths[1:], 1)\n",
    "        scores = scores.squeeze(-1)\n",
    "\n",
    "        return scores, paths\n",
    "\n",
    "\n",
    "def log_sum_exp(tensor, dim=0):\n",
    "    \"\"\"LogSumExp operation.\"\"\"\n",
    "    m, _ = torch.max(tensor, dim)\n",
    "    m_exp = m.unsqueeze(-1).expand_as(tensor)\n",
    "    return m + torch.log(torch.sum(torch.exp(tensor - m_exp), dim))\n",
    "\n",
    "\n",
    "def test():\n",
    "    torch.manual_seed(2)\n",
    "    logits = torch.tensor([[[1.2, 2.1], [2.8, 2.1], [2.2, -2.1]], [[4.1, 2.2], [2.8, 2.1], [2.2, -2.1]]])  # 2, 3, 2\n",
    "    predict_mask = torch.tensor([[1, 1, 0], [1, 0, 0]])  # 2, 3\n",
    "    labels = torch.tensor([[1, 0, 0], [0, 1, 1]])  # 2, 3\n",
    "\n",
    "    crf = CRF(4)\n",
    "    logits = crf.pad_logits(logits)\n",
    "    norm_score = crf.calc_norm_score(logits, predict_mask)\n",
    "    print(norm_score)\n",
    "    gold_score = crf.calc_gold_score(logits, labels, predict_mask)\n",
    "    print(gold_score)\n",
    "    loglik = gold_score - norm_score\n",
    "    print(loglik)\n",
    "    print(crf.viterbi_decode(logits, predict_mask))\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_sizes, padding):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(1, num_filters, kernel_sizes, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (batch * seq_len, 1, max_word_len, char_embed_size)\n",
    "        :return: (batch * seq_len, num_filters)\n",
    "        '''\n",
    "        x = self.conv(x).squeeze(-1)  # (batch * seq_len, num_filters, max_word_len)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(-1)  # (batch * seq_len, num_filters)\n",
    "        return x_max\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.1, layer_num=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_num = layer_num\n",
    "        if layer_num == 1:\n",
    "            self.bilstm = nn.LSTM(input_size, hidden_size // 2, layer_num, batch_first=True, bidirectional=True)\n",
    "\n",
    "        else:\n",
    "            self.bilstm = nn.LSTM(input_size, hidden_size // 2, layer_num, batch_first=True, dropout=dropout_rate,\n",
    "                                  bidirectional=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, p in self.bilstm._parameters.items():\n",
    "            if p.dim() > 1:\n",
    "                bias = math.sqrt(6 / (p.size(0) / 4 + p.size(1)))\n",
    "                nn.init.uniform_(p, -bias, bias)\n",
    "            else:\n",
    "                p.data.zero_()\n",
    "                # This is the range of indices for our forget gates for each LSTM cell\n",
    "                p.data[self.hidden_size // 2: self.hidden_size] = 1\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        '''\n",
    "        :param x: (batch, seq_len, input_size)\n",
    "        :param lens: (batch, )\n",
    "        :return: (batch, seq_len, hidden_size)\n",
    "        '''\n",
    "        ordered_lens, index = lens.sort(descending=True)\n",
    "        ordered_x = x[index]\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(ordered_x, ordered_lens, batch_first=True)\n",
    "        packed_output, _ = self.bilstm(packed_x)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        recover_index = index.argsort()\n",
    "        output = output[recover_index]\n",
    "        return output\n",
    "\n",
    "\n",
    "class SoftmaxDecoder(nn.Module):\n",
    "    def __init__(self, label_size, input_dim):\n",
    "        super(SoftmaxDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.label_size = label_size\n",
    "        self.linear = torch.nn.Linear(input_dim, label_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        bias = math.sqrt(6 / (self.linear.weight.size(0) + self.linear.weight.size(1)))\n",
    "        nn.init.uniform_(self.linear.weight, -bias, bias)\n",
    "\n",
    "    def forward_model(self, inputs):\n",
    "        batch_size, seq_len, input_dim = inputs.size()\n",
    "        output = inputs.contiguous().view(-1, self.input_dim)\n",
    "        output = self.linear(output)\n",
    "        output = output.view(batch_size, seq_len, self.label_size)\n",
    "        return output\n",
    "\n",
    "    def forward(self, inputs, lens, label_ids=None):\n",
    "        logits = self.forward_model(inputs)\n",
    "        p = torch.nn.functional.softmax(logits, -1)  # (batch_size, max_seq_len, num_labels)\n",
    "        predict_mask = (torch.arange(inputs.size(1)).expand(len(lens), inputs.size(1))).to(lens.device) < lens.unsqueeze(1)\n",
    "        if label_ids is not None:\n",
    "            # cross entropy loss\n",
    "            p = torch.nn.functional.softmax(logits, -1)  # (batch_size, max_seq_len, num_labels)\n",
    "            one_hot_labels = torch.eye(self.label_size)[label_ids].type_as(p)\n",
    "            losses = -torch.log(torch.sum(one_hot_labels * p, -1))  # (batch_size, max_seq_len)\n",
    "            masked_losses = torch.masked_select(losses, predict_mask)  # (batch_sum_real_len)\n",
    "            return masked_losses.sum()\n",
    "        else:\n",
    "            return torch.argmax(logits, -1), p\n",
    "\n",
    "class CRFDecoder(nn.Module):\n",
    "    def __init__(self, label_size, input_dim):\n",
    "        super(CRFDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                                out_features=label_size)\n",
    "        self.crf = CRF(label_size + 2)\n",
    "        self.label_size = label_size\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        bias = math.sqrt(6 / (self.linear.weight.size(0) + self.linear.weight.size(1)))\n",
    "        nn.init.uniform_(self.linear.weight, -bias, bias)\n",
    "\n",
    "    def forward_model(self, inputs):\n",
    "        batch_size, seq_len, input_dim = inputs.size()\n",
    "        output = inputs.contiguous().view(-1, self.input_dim)\n",
    "        output = self.linear(output)\n",
    "        output = output.view(batch_size, seq_len, self.label_size)\n",
    "        return output\n",
    "\n",
    "    def forward(self, inputs, lens, labels=None):\n",
    "        '''\n",
    "        :param inputs:(batch_size, max_seq_len, input_dim)\n",
    "        :param predict_mask:(batch_size, max_seq_len)\n",
    "        :param labels:(batch_size, max_seq_len)\n",
    "        :return: if labels is None, return preds(batch_size, max_seq_len) and p(batch_size, max_seq_len, num_labels);\n",
    "                 else return loss (scalar).\n",
    "        '''\n",
    "        logits = self.forward_model(inputs)  # (batch_size, max_seq_len, num_labels)\n",
    "        p = torch.nn.functional.softmax(logits, -1)  # (batch_size, max_seq_len, num_labels)\n",
    "        logits = self.crf.pad_logits(logits)\n",
    "        predict_mask = (torch.arange(inputs.size(1)).expand(len(lens), inputs.size(1))).to(lens.device) < lens.unsqueeze(1)\n",
    "        if labels is None:\n",
    "            _, preds = self.crf.viterbi_decode(logits, predict_mask)\n",
    "            return preds, p\n",
    "        return self.neg_log_likehood(logits, predict_mask, labels)\n",
    "\n",
    "    def neg_log_likehood(self, logits, predict_mask, labels):\n",
    "        norm_score = self.crf.calc_norm_score(logits, predict_mask)\n",
    "        gold_score = self.crf.calc_gold_score(logits, labels, predict_mask)\n",
    "        loglik = gold_score - norm_score\n",
    "        return -loglik.sum()\n",
    "\n",
    "\n",
    "class NER_Model(nn.Module):\n",
    "    def __init__(self, word_embed, char_embed,\n",
    "                 num_labels, hidden_size, dropout_rate=(0.33, 0.5, (0.33, 0.5)),\n",
    "                 lstm_layer_num=1, kernel_step=3, char_out_size=100, use_char=False,\n",
    "                 freeze=False, use_crf=True):\n",
    "        super(NER_Model, self).__init__()\n",
    "        self.word_embed = nn.Embedding.from_pretrained(word_embed, freeze)\n",
    "        self.word_embed_size = word_embed.size(-1)\n",
    "        self.use_char = use_char\n",
    "        if use_char:\n",
    "            self.char_embed = nn.Embedding.from_pretrained(char_embed, freeze)\n",
    "            self.char_embed_size = char_embed.size(-1)\n",
    "            self.charcnn = CharCNN(char_out_size, (kernel_step, self.char_embed_size), (2, 0))\n",
    "            self.bilstm = BiLSTM(char_out_size + self.word_embed_size, hidden_size, dropout_rate[2][1], lstm_layer_num)\n",
    "        else:\n",
    "            self.bilstm = BiLSTM(self.word_embed_size, hidden_size, dropout_rate[2][1], lstm_layer_num)\n",
    "\n",
    "        self.embed_dropout = nn.Dropout(dropout_rate[0])\n",
    "        self.out_dropout = nn.Dropout(dropout_rate[1])\n",
    "        self.rnn_in_dropout = nn.Dropout(dropout_rate[2][0])\n",
    "\n",
    "        if use_crf:\n",
    "            self.decoder = CRFDecoder(num_labels, hidden_size)\n",
    "        else:\n",
    "            self.decoder = SoftmaxDecoder(num_labels, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, word_ids, char_ids, lens, label_ids=None):\n",
    "        '''\n",
    "\n",
    "        :param word_ids: (batch_size, max_seq_len)\n",
    "        :param char_ids: (batch_size, max_seq_len, max_word_len)\n",
    "        :param predict_mask: (batch_size, max_seq_len)\n",
    "        :param label_ids: (batch_size, max_seq_len, max_word_len)\n",
    "        :return: if labels is None, return preds(batch_size, max_seq_len) and p(batch_size, max_seq_len, num_labels);\n",
    "                 else return loss (scalar).\n",
    "        '''\n",
    "        word_embed = self.word_embed(word_ids)\n",
    "        if self.char_embed:\n",
    "            # reshape char_embed and apply to CNN\n",
    "            char_embed = self.char_embed(char_ids).reshape(-1, char_ids.size(-1), self.char_embed_size).unsqueeze(1)\n",
    "            char_embed = self.embed_dropout(\n",
    "                char_embed)  # a dropout layer applied before character embeddings are input to CNN.\n",
    "            char_embed = self.charcnn(char_embed)\n",
    "            char_embed = char_embed.reshape(char_ids.size(0), char_ids.size(1), -1)\n",
    "            embed = torch.cat([word_embed, char_embed], -1)\n",
    "        else:\n",
    "            embed = word_embed\n",
    "        x = self.rnn_in_dropout(embed)\n",
    "        hidden = self.bilstm(x, lens)  # (batch_size, max_seq_len, hidden_size)\n",
    "        hidden = self.out_dropout(hidden)\n",
    "        return self.decoder(hidden, lens, label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_data(input_file):\n",
    "    \"\"\"Reads a BIO data.\"\"\"\n",
    "    with open(input_file) as f:\n",
    "        lines = []\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            contends = line.strip()\n",
    "            # if contends.startswith(\"-DOCSTART-\"):\n",
    "            #     continue\n",
    "            if len(contends) == 0:\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                lines.append([words, [list(word) for word in words], labels])\n",
    "                words = []\n",
    "                labels = []\n",
    "                continue\n",
    "            tokens = line.strip().split(' ')\n",
    "            assert (len(tokens) == 4)\n",
    "            word = tokens[0]\n",
    "            label = tokens[-1]\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "        return lines\n",
    "\n",
    "\n",
    "class ConllDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, word_field, char_field, label_field, datafile, **kwargs):\n",
    "        fields = [(\"word\", word_field), (\"char\", char_field), (\"label\", label_field)]\n",
    "        datas = read_data(datafile)\n",
    "        examples = []\n",
    "        for word, char, label in datas:\n",
    "            examples.append(data.Example.fromlist([word, char, label], fields))\n",
    "        super(ConllDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "\n",
    "def unk_init(x):\n",
    "    dim = x.size(-1)\n",
    "    bias = math.sqrt(3.0 / dim)\n",
    "    x.uniform_(-bias, bias)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_char_detail(train, other, embed_vocab=None):\n",
    "    char2type = {}  # type, 1:ootv, 2:ooev, 3:oobv, 4:iv\n",
    "    ootv = 0\n",
    "    ootv_set = set()\n",
    "    ooev = 0\n",
    "    oobv = 0\n",
    "    iv = 0\n",
    "    fuzzy_iv = 0\n",
    "    for sent in other:\n",
    "        for w in sent:\n",
    "            for c in w:\n",
    "                if c not in char2type:\n",
    "                    if c not in train.stoi:\n",
    "                        if embed_vocab and (c in embed_vocab.stoi or c.lower() in embed_vocab.stoi):\n",
    "                            ootv += 1\n",
    "                            ootv_set.add(c)\n",
    "                            char2type[c] = 1\n",
    "                        else:\n",
    "                            oobv += 1\n",
    "                            char2type[c] = 3\n",
    "                    else:\n",
    "                        if embed_vocab and (c in embed_vocab.stoi or c.lower() in embed_vocab.stoi):\n",
    "                            fuzzy_iv += 1 if c.lower() in embed_vocab.stoi else 0\n",
    "                            iv += 1\n",
    "                            char2type[c] = 4\n",
    "                        else:\n",
    "                            ooev += 1\n",
    "                            char2type[c] = 2\n",
    "    print(\"IV {}(fuzzy {})\\nOOTV {}\\nOOEV {}\\nOOBV {}\\n\".format(iv, fuzzy_iv, ootv, ooev, oobv))\n",
    "    return char2type, ootv_set\n",
    "\n",
    "\n",
    "def get_word_detail(train, other, embed_vocab=None):\n",
    "    '''\n",
    "    OOTV words are the ones do not appear in training set but in embedding vocabulary\n",
    "    OOEV words are the ones do not appear in embedding vocabulary but in training set\n",
    "    OOBV words are the ones do not appears in both the training and embedding vocabulary\n",
    "    IV words the ones appears in both the training and embedding vocabulary\n",
    "    '''\n",
    "    word2type = {}  # type, 1:ootv, 2:ooev, 3:oobv, 4:iv\n",
    "    ootv = 0\n",
    "    ootv_set = set()\n",
    "    ooev = 0\n",
    "    oobv = 0\n",
    "    iv = 0\n",
    "    fuzzy_iv = 0\n",
    "    for sent in other:\n",
    "        for w in sent:\n",
    "            if w not in word2type:\n",
    "                if w not in train.stoi:\n",
    "                    if embed_vocab and (w in embed_vocab.stoi or w.lower() in embed_vocab.stoi):\n",
    "                        ootv += 1\n",
    "                        ootv_set.add(w)\n",
    "                        word2type[w] = 1\n",
    "                    else:\n",
    "                        oobv += 1\n",
    "                        word2type[w] = 3\n",
    "                else:\n",
    "                    if embed_vocab and (w in embed_vocab.stoi or w.lower() in embed_vocab.stoi):\n",
    "                        fuzzy_iv += 1 if w not in embed_vocab.stoi else 0\n",
    "                        iv += 1\n",
    "                        word2type[w] = 4\n",
    "                    else:\n",
    "                        ooev += 1\n",
    "                        word2type[w] = 2\n",
    "    print(\"IV {}(fuzzy {})\\nOOTV {}\\nOOEV {}\\nOOBV {}\\n\".format(iv, fuzzy_iv, ootv, ooev, oobv))\n",
    "    return word2type, ootv_set\n",
    "\n",
    "\n",
    "def get_entity_detail(vocab, data, tag2id, embed_vocab=None):\n",
    "    entity2type = {}  # type, 1:ootv, 2:ooev, 3:oobv, 4:iv\n",
    "    ootv = 0  # every word of the entity have embedding, but at least one word not in training set\n",
    "    oobv = 0  # an entity is considered as OOBV if there exists at least one word not in training set and at least one word not in embedding vocabulary\n",
    "    ooev = 0  # every word of the entity is in training set, but at least one word not have embedding\n",
    "    iv = 0\n",
    "    for ex in data.examples:\n",
    "        ens = get_chunks(ex.label, tag2id, id_format=False)\n",
    "        for e in ens:\n",
    "            if e not in entity2type:\n",
    "                entity_words = [ex.word[ix] for ix in range(e[1], e[2])]\n",
    "                entity_word = ' '.join(entity_words)\n",
    "                not_in_vocab = len(list(filter(lambda w: w not in vocab.stoi, entity_words)))\n",
    "                if embed_vocab:\n",
    "                    not_in_embed = len(list(\n",
    "                        filter(lambda w: w not in embed_vocab.stoi and w.lower() not in embed_vocab.stoi,\n",
    "                               entity_words)))\n",
    "                if not_in_vocab > 0:\n",
    "                    if embed_vocab and not_in_embed == 0:\n",
    "                        ootv += 1\n",
    "                        entity2type[entity_word] = 1\n",
    "                    else:\n",
    "                        oobv += 1\n",
    "                        entity2type[entity_word] = 3\n",
    "                else:\n",
    "                    if embed_vocab and not_in_embed == 0:\n",
    "                        iv += 1\n",
    "                        entity2type[entity_word] = 4\n",
    "                    else:\n",
    "                        ooev += 1\n",
    "                        entity2type[entity_word] = 2\n",
    "\n",
    "    print(\"IV {}\\nOOTV {}\\nOOEV {}\\nOOBV {}\\n\".format(iv, ootv, ooev, oobv))\n",
    "    return entity2type\n",
    "\n",
    "\n",
    "def extend(vocab, v, sort=False):\n",
    "    words = sorted(v) if sort else v\n",
    "    for w in words:\n",
    "        if w not in vocab.stoi:\n",
    "            vocab.itos.append(w)\n",
    "            vocab.stoi[w] = len(vocab.itos) - 1\n",
    "\n",
    "\n",
    "def get_entities(vocab, data, tag2id):\n",
    "    entities = {}\n",
    "    unk = 0\n",
    "    conflict = 0\n",
    "    for ex in data.examples:\n",
    "        ens = get_chunks(ex.label, tag2id, id_format=False)\n",
    "        for e in ens:\n",
    "            entity_words = [ex.word[ix] if ex.word[ix] in vocab.stoi else vocab.UNK for ix in range(e[1], e[2])]\n",
    "            entities.setdefault(' '.join(entity_words), set())\n",
    "            entities[' '.join(entity_words)].add(e[0])\n",
    "            if vocab.UNK in entity_words:\n",
    "                unk += 1\n",
    "            if len(entities[' '.join(entity_words)]) == 2:\n",
    "                conflict += 1\n",
    "    print(\"entities contains `UNK` %d\\nconflict entities %d\\nall entities: %d\\n\" % (unk, conflict, len(entities)))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def load_iters(word_embed_size, word_vectors, char_embedding_size, char_vectors, batch_size=32, device=\"cpu\",\n",
    "               data_path='data', word2lower=True):\n",
    "    zero_char_in_word = lambda ex: [re.sub('\\d', '0', w) for w in ex]\n",
    "    zero_char = lambda w: [re.sub('\\d', '0', c) for c in w]\n",
    "\n",
    "    WORD_TEXT = data.Field(lower=word2lower, batch_first=True, include_lengths=True,\n",
    "                           preprocessing=zero_char_in_word)\n",
    "    CHAR_NESTING = data.Field(tokenize=list, preprocessing=zero_char)  # process a word in char list\n",
    "    CHAR_TEXT = data.NestedField(CHAR_NESTING)\n",
    "    LABEL = data.Field(unk_token=None, pad_token=\"O\", batch_first=True)\n",
    "\n",
    "    train_data = ConllDataset(WORD_TEXT, CHAR_TEXT, LABEL, os.path.join(data_path, \"train.txt\"))\n",
    "    dev_data = ConllDataset(WORD_TEXT, CHAR_TEXT, LABEL, os.path.join(data_path, \"dev.txt\"))\n",
    "    test_data = ConllDataset(WORD_TEXT, CHAR_TEXT, LABEL, os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "    print(\"train sentence num / total word num: %d/%d\" % (\n",
    "        len(train_data.examples), np.array([len(_.word) for _ in train_data.examples]).sum()))\n",
    "    print(\"dev sentence num / total word num: %d/%d\" % (\n",
    "        len(dev_data.examples), np.array([len(_.word) for _ in dev_data.examples]).sum()))\n",
    "    print(\"test sentence num / total word num: %d/%d\" % (\n",
    "        len(test_data.examples), np.array([len(_.word) for _ in test_data.examples]).sum()))\n",
    "\n",
    "    LABEL.build_vocab(train_data.label)\n",
    "    WORD_TEXT.build_vocab(train_data.word, max_size=50000, min_freq=1)\n",
    "    CHAR_TEXT.build_vocab(train_data.char, max_size=50000, min_freq=1)\n",
    "\n",
    "    # ------------------- word oov analysis-----------------------\n",
    "    print('*' * 50 + ' unique words details of dev set ' + '*' * 50)\n",
    "    dev_word2type, dev_ootv_set = get_word_detail(WORD_TEXT.vocab, dev_data.word, word_vectors)\n",
    "    print('#' * 110)\n",
    "    print('*' * 50 + ' unique words details of test set ' + '*' * 50)\n",
    "    test_word2type, test_ootv_set = get_word_detail(WORD_TEXT.vocab, test_data.word, word_vectors)\n",
    "    print('#' * 110)\n",
    "    WORD_TEXT.vocab.dev_word2type = dev_word2type\n",
    "    WORD_TEXT.vocab.test_word2type = test_word2type\n",
    "\n",
    "    # ------------------- entity oov analysis-----------------------\n",
    "    print('*' * 50 + ' get train entities ' + '*' * 50)\n",
    "    train_entities = get_entities(WORD_TEXT.vocab, train_data, LABEL.vocab.stoi)\n",
    "    print('#' * 110)\n",
    "    print('*' * 50 + ' get dev entities ' + '*' * 50)\n",
    "    dev_entity2type = get_entity_detail(WORD_TEXT.vocab, dev_data, LABEL.vocab.stoi, word_vectors)\n",
    "    print('#' * 110)\n",
    "    print('*' * 50 + ' get test entities ' + '*' * 50)\n",
    "    test_entity2type = get_entity_detail(WORD_TEXT.vocab, test_data, LABEL.vocab.stoi, word_vectors)\n",
    "    print('#' * 110)\n",
    "    WORD_TEXT.vocab.dev_entity2type = dev_entity2type\n",
    "    WORD_TEXT.vocab.test_entity2type = test_entity2type\n",
    "\n",
    "    # ------------------- extend word vocab with ootv words -----------------------\n",
    "    print('*' * 50 + 'extending ootv words to vocab' + '*' * 50)\n",
    "    ootv = list(dev_ootv_set.union(test_ootv_set))\n",
    "    extend(WORD_TEXT.vocab, ootv)\n",
    "    print('extended %d words' % len(ootv))\n",
    "    print('#' * 110)\n",
    "\n",
    "    # ------------------- generate word embedding -----------------------\n",
    "    vectors_to_use = unk_init(torch.zeros((len(WORD_TEXT.vocab), word_embed_size)))\n",
    "    if word_vectors is not None:\n",
    "        vectors_to_use = get_vectors(vectors_to_use, WORD_TEXT.vocab, word_vectors)\n",
    "    WORD_TEXT.vocab.vectors = vectors_to_use\n",
    "\n",
    "    # ------------------- char oov analysis-----------------------\n",
    "    print('*' * 50 + ' unique chars details of dev set ' + '*' * 50)\n",
    "    dev_char2type, dev_ootv_set = get_char_detail(CHAR_TEXT.vocab, dev_data.char, char_vectors)\n",
    "    print('#' * 110)\n",
    "    print('*' * 50 + ' unique chars details of test set ' + '*' * 50)\n",
    "    test_char2type, test_ootv_set = get_char_detail(CHAR_TEXT.vocab, test_data.char, char_vectors)\n",
    "    print('#' * 110)\n",
    "    CHAR_TEXT.vocab.dev_char2type = dev_char2type\n",
    "    CHAR_TEXT.vocab.test_char2type = test_char2type\n",
    "\n",
    "    # ------------------- extend char vocab with ootv chars -----------------------\n",
    "    print('*' * 50 + 'extending ootv chars to vocab' + '*' * 50)\n",
    "    ootv = list(dev_ootv_set.union(test_ootv_set))\n",
    "    extend(CHAR_TEXT.vocab, ootv)\n",
    "    print('extended %d chars' % len(ootv))\n",
    "    print('#' * 110)\n",
    "\n",
    "    # ------------------- generate char embedding -----------------------\n",
    "    vectors_to_use = unk_init(torch.zeros((len(CHAR_TEXT.vocab), char_embedding_size)))\n",
    "    if char_vectors is not None:\n",
    "        vectors_to_use = get_vectors(vectors_to_use, CHAR_TEXT.vocab, char_vectors)\n",
    "    CHAR_TEXT.vocab.vectors = vectors_to_use\n",
    "\n",
    "    print(\"word vocab size: \", len(WORD_TEXT.vocab))\n",
    "    print(\"char vocab size: \", len(CHAR_TEXT.vocab))\n",
    "    print(\"label vocab size: \", len(LABEL.vocab))\n",
    "\n",
    "    train_iter = BucketIterator(train_data, batch_size=batch_size, device=device, sort_key=lambda x: len(x.word),\n",
    "                                sort_within_batch=True, repeat=False, shuffle=True)\n",
    "    dev_iter = Iterator(dev_data, batch_size=batch_size, device=device, sort=False, sort_within_batch=False,\n",
    "                        repeat=False, shuffle=False)\n",
    "    test_iter = Iterator(test_data, batch_size=batch_size, device=device, sort=False, sort_within_batch=False,\n",
    "                         repeat=False, shuffle=False)\n",
    "    return train_iter, dev_iter, test_iter, WORD_TEXT, CHAR_TEXT, LABEL\n",
    "\n",
    "\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
    "    as defined in BIOES\n",
    "\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags, bioes=True, id_format=True):\n",
    "    \"\"\"\n",
    "    Given a sequence of tags, group entities and their position\n",
    "    \"\"\"\n",
    "    if not id_format:\n",
    "        seq = [tags[_] for _ in seq]\n",
    "\n",
    "    # We assume by default the tags lie outside a named entity\n",
    "    default = tags[\"O\"]\n",
    "\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    chunk_class, chunk_type, chunk_start = None, None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        if tok == default and (chunk_class in ([\"E\", \"S\"] if bioes else [\"B\", \"I\"])):\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_class, chunk_type, chunk_start = \"O\", None, None\n",
    "\n",
    "        if tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                # Initialize chunk for each entity\n",
    "                chunk_class, chunk_type, chunk_start = tok_chunk_class, tok_chunk_type, i\n",
    "            else:\n",
    "                if bioes:\n",
    "                    if chunk_class in [\"E\", \"S\"]:\n",
    "                        chunk = (chunk_type, chunk_start, i)\n",
    "                        chunks.append(chunk)\n",
    "                        if tok_chunk_class in [\"B\", \"S\"]:\n",
    "                            chunk_class, chunk_type, chunk_start = tok_chunk_class, tok_chunk_type, i\n",
    "                        else:\n",
    "                            chunk_class, chunk_type, chunk_start = None, None, None\n",
    "                    elif tok_chunk_type == chunk_type and chunk_class in [\"B\", \"I\"]:\n",
    "                        chunk_class = tok_chunk_class\n",
    "                    else:\n",
    "                        chunk_class, chunk_type = None, None\n",
    "                else:  # BIO schema\n",
    "                    if tok_chunk_class == \"B\":\n",
    "                        chunk = (chunk_type, chunk_start, i)\n",
    "                        chunks.append(chunk)\n",
    "                        chunk_class, chunk_type, chunk_start = tok_chunk_class, tok_chunk_type, i\n",
    "                    else:\n",
    "                        chunk_class, chunk_type = None, None\n",
    "\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectors(embed, vocab, pretrain_embed_vocab):\n",
    "    oov = 0\n",
    "    for i, word in enumerate(vocab.itos):\n",
    "        index = pretrain_embed_vocab.stoi.get(word, None)  # digit or None\n",
    "        if index is None:\n",
    "            if word.lower() in pretrain_embed_vocab.stoi:\n",
    "                index = pretrain_embed_vocab.stoi[word.lower()]\n",
    "        if index:\n",
    "            embed[i] = pretrain_embed_vocab.vectors[index]\n",
    "        else:\n",
    "            oov += 1\n",
    "    print('train vocab oov %d \\ntrain vocab + dev ootv + test ootv: %d' % (oov, len(vocab.stoi)))\n",
    "    return embed\n",
    "\n",
    "\n",
    "def test_get_chunks():\n",
    "    print(get_chunks([4, 2, 1, 2, 3, 3],\n",
    "                     {'O': 0, \"B-PER\": 1, \"I-PER\": 2, \"E-PER\": 3, \"S-PER\": 4}))\n",
    "    print(get_chunks([\"S-PER\", \"I-PER\", \"B-PER\", \"I-PER\", \"E-PER\", \"E-PER\"],\n",
    "                     {'O': 0, \"B-PER\": 1, \"I-PER\": 2, \"E-PER\": 3, \"S-PER\": 4}, id_format=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import Vectors\n",
    "import codecs\n",
    "\n",
    "torch.manual_seed(1)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_PATH = \"out_data\"\n",
    "PREDICT_OUT_FILE = \"res.txt\"\n",
    "BEST_MODEL = \"best_model.ckpt\"\n",
    "BATCH_SIZE = 32\n",
    "LOWER_CASE = False\n",
    "EPOCHS = 20\n",
    "\n",
    "# embedding\n",
    "WORD_VECTORS = None\n",
    "# WORD_VECTORS = Vectors('glove.6B.100d.txt', '../../embeddings/glove.6B')\n",
    "WORD_EMBEDDING_SIZE = 100\n",
    "CHAR_VECTORS = None\n",
    "CHAR_EMBEDDING_SIZE = 30  # the input char embedding to CNN\n",
    "FREEZE_EMBEDDING = False\n",
    "\n",
    "# SGD parameters\n",
    "LEARNING_RATE = 0.015\n",
    "DECAY_RATE = 0.05\n",
    "MOMENTUM = 0.9\n",
    "CLIP = 5\n",
    "PATIENCE = 5\n",
    "\n",
    "# network parameters\n",
    "HIDDEN_SIZE = 400  # every LSTM's(forward and backward) hidden size is half of HIDDEN_SIZE\n",
    "LSTM_LAYER_NUM = 1\n",
    "DROPOUT_RATE = (0.5, 0.5, (0.5, 0.5))  # after embed layer, other case, (input to rnn, between rnn layers)\n",
    "USE_CHAR = True  # use char level information\n",
    "N_FILTERS = 30  # the output char embedding from CNN\n",
    "KERNEL_STEP = 3  # n-gram size of CNN\n",
    "USE_CRF = True\n",
    "\n",
    "\n",
    "def train(train_iter, dev_iter, optimizer):\n",
    "    best_dev_f1 = -1\n",
    "    patience_counter = 0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_iter.init_epoch()\n",
    "        for i, batch in enumerate(tqdm(train_iter)):\n",
    "            words, lens = batch.word\n",
    "            labels = batch.label\n",
    "            if i < 2:\n",
    "                tqdm.write(' '.join([WORD.vocab.itos[i] for i in words[0]]))\n",
    "                tqdm.write(' '.join([LABEL.vocab.itos[i] for i in labels[0]]))\n",
    "            model.zero_grad()\n",
    "            loss = model(words, batch.char, lens, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "        tqdm.write(\"Epoch: %d, Train Loss: %d\" % (epoch, total_loss))\n",
    "\n",
    "        lr = LEARNING_RATE / (1 + DECAY_RATE * epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        dev_f1 = eval(dev_iter, \"Dev\", epoch)\n",
    "        if dev_f1 < best_dev_f1:\n",
    "            patience_counter += 1\n",
    "            tqdm.write(\"No improvement, patience: %d/%d\" % (patience_counter, PATIENCE))\n",
    "        else:\n",
    "            best_dev_f1 = dev_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), BEST_MODEL)\n",
    "            tqdm.write(\"New best model, saved to best_model.ckpt, patience: 0/%d\" % PATIENCE)\n",
    "        if patience_counter >= PATIENCE:\n",
    "            tqdm.write(\"Early stopping: patience limit reached, stopping...\")\n",
    "            break\n",
    "\n",
    "\n",
    "def eval(data_iter, name, epoch=None, best_model=None):\n",
    "    if best_model:\n",
    "        model.load_state_dict(torch.load(best_model))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        res = {'ootv': [0, 0, 0], 'ooev': [0, 0, 0], 'oobv': [0, 0, 0], 'iv': [0, 0, 0],\n",
    "               'total': [0, 0, 0]}  # e.g. 'iv':[correct_preds, total_preds, total_correct]\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            words, lens = batch.word\n",
    "            labels = batch.label\n",
    "            predicted_seq, _ = model(words, batch.char, lens)  # predicted_seq : (batch_size, seq_len)\n",
    "            loss = model(words, batch.char, lens, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            orig_text = [e.word for e in data_iter.dataset.examples[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]]\n",
    "            for text, ground_truth_id, predicted_id, len_ in zip(orig_text, labels.cpu().numpy(),\n",
    "                                                                 predicted_seq.cpu().numpy(),\n",
    "                                                                 lens.cpu().numpy()):\n",
    "                lab_chunks = set(get_chunks(ground_truth_id[:len_], LABEL.vocab.stoi))\n",
    "                lab_pred_chunks = set(get_chunks(predicted_id[:len_], LABEL.vocab.stoi))\n",
    "\n",
    "                for chunk in list(lab_chunks):\n",
    "                    entity_word = ' '.join([text[ix] for ix in range(chunk[1], chunk[2])])\n",
    "                    # type, 1:ootv, 2:ooev, 3:oobv, 4:iv\n",
    "                    entity_type = WORD.vocab.dev_entity2type[entity_word] if name == \"Dev\" else \\\n",
    "                    WORD.vocab.test_entity2type[entity_word]\n",
    "                    if entity_type == 1:\n",
    "                        if chunk in lab_pred_chunks:\n",
    "                            res['ootv'][0] += 1\n",
    "                        res['ootv'][2] += 1\n",
    "                    elif entity_type == 2:\n",
    "                        if chunk in lab_pred_chunks:\n",
    "                            res['ooev'][0] += 1\n",
    "                        res['ooev'][2] += 1\n",
    "                    elif entity_type == 3:\n",
    "                        if chunk in lab_pred_chunks:\n",
    "                            res['oobv'][0] += 1\n",
    "                        res['oobv'][2] += 1\n",
    "                    else:\n",
    "                        if chunk in lab_pred_chunks:\n",
    "                            res['iv'][0] += 1\n",
    "                        res['iv'][2] += 1\n",
    "                    if chunk in lab_pred_chunks:\n",
    "                        res['total'][0] += 1\n",
    "                    res['total'][2] += 1\n",
    "                for chunk in list(lab_pred_chunks):\n",
    "                    entity_word = ' '.join([text[ix] for ix in range(chunk[1], chunk[2])])\n",
    "                    # type, 1:ootv, 2:ooev, 3:oobv, 4:iv\n",
    "                    entity_type = WORD.vocab.dev_entity2type.get(entity_word, None) if name == \"Dev\" else \\\n",
    "                        WORD.vocab.test_entity2type.get(entity_word, None)\n",
    "                    if entity_type == 1:\n",
    "                        res['ootv'][1] += 1\n",
    "                    elif entity_type == 2:\n",
    "                        res['ooev'][1] += 1\n",
    "                    elif entity_type == 3:\n",
    "                        res['oobv'][1] += 1\n",
    "                    elif entity_type == 4:\n",
    "                        res['iv'][1] += 1\n",
    "                    res['total'][1] += 1\n",
    "\n",
    "        # Calculating the F1-Score\n",
    "        for k, v in res.items():\n",
    "            p = v[0] / v[1] if v[1] != 0 else 0\n",
    "            r = v[0] / v[2] if v[2] != 0 else 0\n",
    "            micro_F1 = 2 * p * r / (p + r) if (p + r) != 0 else 0\n",
    "            if epoch is not None:\n",
    "                tqdm.write(\n",
    "                    \"Epoch: %d, %s, %s Entity Micro F1: %.3f, Loss %.3f\" % (epoch, name, k, micro_F1, total_loss))\n",
    "            else:\n",
    "                tqdm.write(\n",
    "                    \"%s, %s Entity Micro F1: %.3f, Loss %.3f\" % (name, k, micro_F1, total_loss))\n",
    "    return micro_F1\n",
    "\n",
    "\n",
    "def predict(data_iter, out_file):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        gold_seqs = []\n",
    "        predicted_seqs = []\n",
    "        word_seqs = []\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            words, lens = batch.word\n",
    "            predicted_seq, _ = model(words, batch.char, lens)  # predicted_seq : (batch_size, seq_len)\n",
    "            gold_seqs.extend(batch.label.tolist())\n",
    "            predicted_seqs.extend(predicted_seq.tolist())\n",
    "            word_seqs.extend(words.tolist())\n",
    "        write_predicted_labels(out_file, data_iter.dataset.examples, word_seqs, LABEL.vocab.itos, gold_seqs,\n",
    "                               predicted_seqs)\n",
    "\n",
    "\n",
    "def write_predicted_labels(output_file, orig_text, word_ids, id2label, gold_seq, predicted_seq):\n",
    "    with codecs.open(output_file, 'w', encoding='utf-8') as writer:\n",
    "        for text, wids, predict, gold in zip(orig_text, word_ids, predicted_seq, gold_seq):\n",
    "            ix = 0\n",
    "            for w_id, p_id, g_id in zip(wids, predict, gold):\n",
    "                if w_id == pad_idx: break\n",
    "                output_line = ' '.join([text.word[ix], id2label[g_id], id2label[p_id]])\n",
    "                writer.write(output_line + '\\n')\n",
    "                ix += 1\n",
    "            writer.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France NNP I-NP S-LOC\n",
      "critics NNS I-NP O\n",
      "managed VBD B-VP O\n"
     ]
    }
   ],
   "source": [
    "train_data = read_file('./data/train.txt')\n",
    "convert(train_data, './out_data/train.txt')\n",
    "dev_data = read_file('./data/dev.txt')\n",
    "convert(dev_data, './out_data/dev.txt')\n",
    "test_data = read_file('./data/test.txt')\n",
    "convert(test_data, './out_data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentence num / total word num: 14737/172400\n",
      "dev sentence num / total word num: 3412/43461\n",
      "test sentence num / total word num: 3646/38944\n",
      "************************************************** unique words details of dev set **************************************************\n",
      "IV 0(fuzzy 0)\n",
      "OOTV 0\n",
      "OOEV 4334\n",
      "OOBV 1565\n",
      "\n",
      "##############################################################################################################\n",
      "************************************************** unique words details of test set **************************************************\n",
      "IV 0(fuzzy 0)\n",
      "OOTV 0\n",
      "OOEV 3874\n",
      "OOBV 1513\n",
      "\n",
      "##############################################################################################################\n",
      "************************************************** get train entities **************************************************\n",
      "entities contains `UNK` 0\n",
      "conflict entities 2\n",
      "all entities: 1042\n",
      "\n",
      "##############################################################################################################\n",
      "************************************************** get dev entities **************************************************\n",
      "IV 0\n",
      "OOTV 0\n",
      "OOEV 122\n",
      "OOBV 161\n",
      "\n",
      "##############################################################################################################\n",
      "************************************************** get test entities **************************************************\n",
      "IV 0\n",
      "OOTV 0\n",
      "OOEV 119\n",
      "OOBV 140\n",
      "\n",
      "##############################################################################################################\n",
      "**************************************************extending ootv words to vocab**************************************************\n",
      "extended 0 words\n",
      "##############################################################################################################\n",
      "************************************************** unique chars details of dev set **************************************************\n",
      "IV 0(fuzzy 0)\n",
      "OOTV 0\n",
      "OOEV 71\n",
      "OOBV 0\n",
      "\n",
      "##############################################################################################################\n",
      "************************************************** unique chars details of test set **************************************************\n",
      "IV 0(fuzzy 0)\n",
      "OOTV 0\n",
      "OOEV 73\n",
      "OOBV 1\n",
      "\n",
      "##############################################################################################################\n",
      "**************************************************extending ootv chars to vocab**************************************************\n",
      "extended 0 chars\n",
      "##############################################################################################################\n",
      "word vocab size:  12940\n",
      "char vocab size:  77\n",
      "label vocab size:  9\n",
      "NER_Model(\n",
      "  (word_embed): Embedding(12940, 100)\n",
      "  (char_embed): Embedding(77, 30)\n",
      "  (charcnn): CharCNN(\n",
      "    (conv): Conv2d(1, 30, kernel_size=(3, 30), stride=(1, 1), padding=(2, 0))\n",
      "  )\n",
      "  (bilstm): BiLSTM(\n",
      "    (bilstm): LSTM(130, 200, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (embed_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (out_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (rnn_in_dropout): Dropout(p=0.5, inplace=False)\n",
      "  (decoder): CRFDecoder(\n",
      "    (linear): Linear(in_features=400, out_features=9, bias=True)\n",
      "    (crf): CRF()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The firm has signed an agreement with the Polish ( ) to manage the issue and plans them to be listed on the bourse 's bond market .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O B-ORG O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:25<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 17524\n",
      "Epoch: 1, Dev, ootv Entity Micro F1: 0.000, Loss 2662.724\n",
      "Epoch: 1, Dev, ooev Entity Micro F1: 0.000, Loss 2662.724\n",
      "Epoch: 1, Dev, oobv Entity Micro F1: 0.000, Loss 2662.724\n",
      "Epoch: 1, Dev, iv Entity Micro F1: 0.000, Loss 2662.724\n",
      "Epoch: 1, Dev, total Entity Micro F1: 0.000, Loss 2662.724\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 00 00 00 , 00 00 00 00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our not signing a new treaty does not mean we are going in for any new kind of weapons , particularly nuclear . \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 10294\n",
      "Epoch: 2, Dev, ootv Entity Micro F1: 0.000, Loss 3001.220\n",
      "Epoch: 2, Dev, ooev Entity Micro F1: 0.179, Loss 3001.220\n",
      "Epoch: 2, Dev, oobv Entity Micro F1: 0.012, Loss 3001.220\n",
      "Epoch: 2, Dev, iv Entity Micro F1: 0.000, Loss 3001.220\n",
      "Epoch: 2, Dev, total Entity Micro F1: 0.088, Loss 3001.220\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diplomats fear that the crisis could cast doubt over the entire election process , which already appears set to confirm 's ethnic partition rather than its reintegration as the peace agreement had planned .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division three - v .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 6184\n",
      "Epoch: 3, Dev, ootv Entity Micro F1: 0.000, Loss 1639.482\n",
      "Epoch: 3, Dev, ooev Entity Micro F1: 0.494, Loss 1639.482\n",
      "Epoch: 3, Dev, oobv Entity Micro F1: 0.135, Loss 1639.482\n",
      "Epoch: 3, Dev, iv Entity Micro F1: 0.000, Loss 1639.482\n",
      "Epoch: 3, Dev, total Entity Micro F1: 0.304, Loss 1639.482\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police were seeking an escaped gunman on Tuesday after an incident in which he fired at a 00-year-old schoolgirl , Radio said .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O B-ORG O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spot riyal against the dollar and riyal interbank deposit rates were mainly steady this week in quiet summer trade , dealers in the kingdom said .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 4434\n",
      "Epoch: 4, Dev, ootv Entity Micro F1: 0.000, Loss 1141.729\n",
      "Epoch: 4, Dev, ooev Entity Micro F1: 0.599, Loss 1141.729\n",
      "Epoch: 4, Dev, oobv Entity Micro F1: 0.340, Loss 1141.729\n",
      "Epoch: 4, Dev, iv Entity Micro F1: 0.000, Loss 1141.729\n",
      "Epoch: 4, Dev, total Entity Micro F1: 0.447, Loss 1141.729\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leading results from round\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" As we have said publicly before , if there is any effort to do so ( hold local elections ) these elections will not be valid , \" he said . \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 3496\n",
      "Epoch: 5, Dev, ootv Entity Micro F1: 0.000, Loss 837.771\n",
      "Epoch: 5, Dev, ooev Entity Micro F1: 0.723, Loss 837.771\n",
      "Epoch: 5, Dev, oobv Entity Micro F1: 0.524, Loss 837.771\n",
      "Epoch: 5, Dev, iv Entity Micro F1: 0.000, Loss 837.771\n",
      "Epoch: 5, Dev, total Entity Micro F1: 0.578, Loss 837.771\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 0000-00-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 2868\n",
      "Epoch: 6, Dev, ootv Entity Micro F1: 0.000, Loss 800.112\n",
      "Epoch: 6, Dev, ooev Entity Micro F1: 0.753, Loss 800.112\n",
      "Epoch: 6, Dev, oobv Entity Micro F1: 0.586, Loss 800.112\n",
      "Epoch: 6, Dev, iv Entity Micro F1: 0.000, Loss 800.112\n",
      "Epoch: 6, Dev, total Entity Micro F1: 0.613, Loss 800.112\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals for , goals against , points ) .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ( ) 00.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 2439\n",
      "Epoch: 7, Dev, ootv Entity Micro F1: 0.000, Loss 877.575\n",
      "Epoch: 7, Dev, ooev Entity Micro F1: 0.746, Loss 877.575\n",
      "Epoch: 7, Dev, oobv Entity Micro F1: 0.523, Loss 877.575\n",
      "Epoch: 7, Dev, iv Entity Micro F1: 0.000, Loss 877.575\n",
      "Epoch: 7, Dev, total Entity Micro F1: 0.602, Loss 877.575\n",
      "No improvement, patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000-00-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:25<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 2020\n",
      "Epoch: 8, Dev, ootv Entity Micro F1: 0.000, Loss 774.694\n",
      "Epoch: 8, Dev, ooev Entity Micro F1: 0.798, Loss 774.694\n",
      "Epoch: 8, Dev, oobv Entity Micro F1: 0.548, Loss 774.694\n",
      "Epoch: 8, Dev, iv Entity Micro F1: 0.000, Loss 774.694\n",
      "Epoch: 8, Dev, total Entity Micro F1: 0.625, Loss 774.694\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", who has been coaching the provincial team , takes over from who took to 00th place in the in .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close of play scores on the first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 1793\n",
      "Epoch: 9, Dev, ootv Entity Micro F1: 0.000, Loss 810.717\n",
      "Epoch: 9, Dev, ooev Entity Micro F1: 0.795, Loss 810.717\n",
      "Epoch: 9, Dev, oobv Entity Micro F1: 0.568, Loss 810.717\n",
      "Epoch: 9, Dev, iv Entity Micro F1: 0.000, Loss 810.717\n",
      "Epoch: 9, Dev, total Entity Micro F1: 0.639, Loss 810.717\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 00 0 0 000 000 00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world 's largest dredger said in March that it was uncertain whether it could hold 0000 full-year profit steady at the previous year 's 00.0 million guilders , but added long-term prospects were good .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 1538\n",
      "Epoch: 10, Dev, ootv Entity Micro F1: 0.000, Loss 785.073\n",
      "Epoch: 10, Dev, ooev Entity Micro F1: 0.814, Loss 785.073\n",
      "Epoch: 10, Dev, oobv Entity Micro F1: 0.589, Loss 785.073\n",
      "Epoch: 10, Dev, iv Entity Micro F1: 0.000, Loss 785.073\n",
      "Epoch: 10, Dev, total Entity Micro F1: 0.655, Loss 785.073\n",
      "New best model, saved to best_model.ckpt, patience: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( ) beat ( ) 0-0 0-0 0-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opened their title defence with a 0-0 win over on Wednesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 1443\n",
      "Epoch: 11, Dev, ootv Entity Micro F1: 0.000, Loss 831.241\n",
      "Epoch: 11, Dev, ooev Entity Micro F1: 0.802, Loss 831.241\n",
      "Epoch: 11, Dev, oobv Entity Micro F1: 0.578, Loss 831.241\n",
      "Epoch: 11, Dev, iv Entity Micro F1: 0.000, Loss 831.241\n",
      "Epoch: 11, Dev, total Entity Micro F1: 0.639, Loss 831.241\n",
      "No improvement, patience: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000-00-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the 's steel-producing arm , slated for a December sale worth an estimated $ 0.0 billion .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 1312\n",
      "Epoch: 12, Dev, ootv Entity Micro F1: 0.000, Loss 783.935\n",
      "Epoch: 12, Dev, ooev Entity Micro F1: 0.798, Loss 783.935\n",
      "Epoch: 12, Dev, oobv Entity Micro F1: 0.605, Loss 783.935\n",
      "Epoch: 12, Dev, iv Entity Micro F1: 0.000, Loss 783.935\n",
      "Epoch: 12, Dev, total Entity Micro F1: 0.638, Loss 783.935\n",
      "No improvement, patience: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 0 through 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( ) vs. ( )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:24<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train Loss: 1098\n",
      "Epoch: 13, Dev, ootv Entity Micro F1: 0.000, Loss 845.494\n",
      "Epoch: 13, Dev, ooev Entity Micro F1: 0.802, Loss 845.494\n",
      "Epoch: 13, Dev, oobv Entity Micro F1: 0.597, Loss 845.494\n",
      "Epoch: 13, Dev, iv Entity Micro F1: 0.000, Loss 845.494\n",
      "Epoch: 13, Dev, total Entity Micro F1: 0.651, Loss 845.494\n",
      "No improvement, patience: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" I was n't happy when I saw the draw .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals for , goals against , points ) .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O O O O O O O O O\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 378/461 [00:20<00:04, 18.20it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7f67244a7f55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMOMENTUM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBEST_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPREDICT_OUT_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8a94cc8a1fdf>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, dev_iter, optimizer)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01349ae9b17a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_ids, char_ids, lens, label_ids)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, max_seq_len, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01349ae9b17a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, lens, labels)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviterbi_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_log_likehood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mneg_log_likehood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-01349ae9b17a>\u001b[0m in \u001b[0;36mneg_log_likehood\u001b[1;34m(self, logits, predict_mask, labels)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mneg_log_likehood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mnorm_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_norm_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[0mgold_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalc_gold_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mloglik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgold_score\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnorm_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aaf435315eff>\u001b[0m in \u001b[0;36mcalc_norm_score\u001b[1;34m(self, logits, predict_mask)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mtrans_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha_exp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogit_exp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha_exp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrans_exp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0malpha_nxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_mask_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, num_labels+2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aaf435315eff>\u001b[0m in \u001b[0;36mlog_sum_exp\u001b[1;34m(tensor, dim)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;34m\"\"\"LogSumExp operation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[0mm_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mm_exp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter, dev_iter, test_iter, WORD, CHAR, LABEL = load_iters(WORD_EMBEDDING_SIZE, WORD_VECTORS,\n",
    "                                                                    CHAR_EMBEDDING_SIZE, CHAR_VECTORS,\n",
    "                                                                    BATCH_SIZE, DEVICE, DATA_PATH, LOWER_CASE)\n",
    "\n",
    "model = NER_Model(WORD.vocab.vectors, CHAR.vocab.vectors, len(LABEL.vocab.stoi), HIDDEN_SIZE, DROPOUT_RATE,\n",
    "                  LSTM_LAYER_NUM,\n",
    "                  KERNEL_STEP, N_FILTERS, USE_CHAR, FREEZE_EMBEDDING, USE_CRF).to(DEVICE)\n",
    "print(model)\n",
    "pad_idx = WORD.vocab.stoi[WORD.pad_token]\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "train(train_iter, dev_iter, optimizer)\n",
    "eval(test_iter, \"Test\", best_model=BEST_MODEL)\n",
    "predict(test_iter, PREDICT_OUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
