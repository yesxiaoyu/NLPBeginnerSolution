{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：基于机器学习的文本分类\n",
    "## 实现基于logistic/softmax regression的文本分类\n",
    "\n",
    "## 1.参考\n",
    "\n",
    "    文本分类\n",
    "    《神经网络与深度学习》 第2/3章\n",
    "## 2.数据集：Classify the sentiment of sentences from the Rotten Tomatoes dataset-best acc score: 0.76526\n",
    "\n",
    "## 3.实现要求：NumPy\n",
    "\n",
    "## 4.需要了解的知识点：\n",
    "\n",
    "    ### 文本特征表示：Bag-of-Word，N-gram\n",
    "    ### 分类器：logistic/softmax regression，损失函数、（随机）梯度下降、特征选择\n",
    "    ### 数据集：训练集/验证集/测试集的划分\n",
    "## 5.实验：\n",
    "\n",
    "    分析不同的特征、损失函数、学习率对最终分类性能的影响\n",
    "    shuffle 、batch、mini-batch\n",
    "## 6.时间：两周\n",
    "-------------------\n",
    "使用numpy导入文件参考\n",
    "https://numpy.org/devdocs/reference/generated/numpy.loadtxt.html?highlight=loadtxt#numpy.loadtxt\n",
    "\n",
    "https://www.runoob.com/numpy/numpy-dtype.html\n",
    "\n",
    "https://blog.csdn.net/qq_38634140/article/details/88650519\n",
    "\n",
    "https://blog.csdn.net/messi_james/article/details/80487389\n",
    "np.set_printoptions(suppress=True) #取消默认的科学计数法\n",
    "#这里的skiprows是指跳过前1行, 如果设置skiprows=2, 就会跳过前两行\n",
    "#Python默认读取的数字的数据类型为双精度浮点数\n",
    "#comment的是指, 如果行的开头为‘#’就会跳过该行\n",
    "#usecols是指只使用0,2两列。usecols=(0,1,2,3,4)\n",
    "x = np.loadtxt(\"./sentiment-analysis-on-movie-reviews/train.tsv\", dtype=np.dtype([('id','S20'), ('txt','S1000'), ('label','S20')]), delimiter='\\t', skiprows=0, usecols=(1, 2, 3), unpack=False)\n",
    "\n",
    "--------------------\n",
    "使用numpy导入文件太费劲了，使用pandas代替。\n",
    "\n",
    "* 词袋模型 https://blog.csdn.net/hao5335156/article/details/80615057\n",
    "* Jack Cui 机器学习 https://cuijiahua.com/blog/2017/11/ml_6_logistic_1.html\n",
    "* https://gitbook.cn/gitchat/column/5cd016a4e30c87051ad2be27/topic/5cd0eb00e30c87051ad2d2b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./sentiment-analysis-on-movie-reviews/train.tsv', sep='\\t')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./sentiment-analysis-on-movie-reviews/test.tsv', sep='\\t')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (156060, 4) & test shape: (66292, 3)\n"
     ]
    }
   ],
   "source": [
    "print('train shape: {} & test shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 借助sklearn划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts = list(train['Phrase'].values)\n",
    "train_labels = train['Sentiment'].values\n",
    "test_texts = list(test['Phrase'].values)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_texts, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_bag_of_words(all_sentences):\n",
    "    vocabSet = []\n",
    "    for sentence in all_sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocabSet:\n",
    "                vocabSet.append(word)\n",
    "    return vocabSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(bow, sentences):\n",
    "    res = []\n",
    "    for sentence in sentences:\n",
    "        feature = [0] * len(bow)\n",
    "        for word in sentence.split():\n",
    "            if word in bow:\n",
    "                feature[bow.index(word)] = 1\n",
    "        res.append(feature)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_text shape: 222352 & all_text0: A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "all_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "print('all_text shape: {} & all_text0: {}'.format(len(all_text), all_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow list longth 21637\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow = sentences_to_bag_of_words(all_text)\n",
    "print('bow list longth', len(bow)) # 21637"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词表的长度是21637，若使用全部单词特征，稀疏矩阵为156060 * 21637维，使用卡方特征选择1000个最好特征单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChiSquare:\n",
    "    def __init__(self, doc_list, doc_labels):\n",
    "        self.total_data, self.total_pos_data, self.total_neg_data = {}, {}, {}\n",
    "        for i, doc in enumerate(doc_list):\n",
    "            if doc_labels[i] == 1:\n",
    "                for word in doc.split():\n",
    "                    self.total_pos_data[word] = self.total_pos_data.get(word, 0) + 1\n",
    "                    self.total_data[word] = self.total_data.get(word, 0) + 1\n",
    "            else:\n",
    "                for word in doc.split():\n",
    "                    self.total_neg_data[word] = self.total_neg_data.get(word, 0) + 1\n",
    "                    self.total_data[word] = self.total_data.get(word, 0) + 1\n",
    "\n",
    "        total_freq = sum(self.total_data.values())\n",
    "        total_pos_freq = sum(self.total_pos_data.values())\n",
    "        # total_neg_freq = sum(self.total_neg_data.values())\n",
    "\n",
    "        self.words = {}\n",
    "        for word, freq in self.total_data.items():\n",
    "            pos_score = self.__calculate(self.total_pos_data.get(word, 0), freq, total_pos_freq, total_freq)\n",
    "            # neg_score = self.__calculate(self.total_neg_data.get(word, 0), freq, total_neg_freq, total_freq)\n",
    "            self.words[word] = pos_score * 2\n",
    "\n",
    "    @staticmethod\n",
    "    def __calculate(n_ii, n_ix, n_xi, n_xx):\n",
    "        n_ii = n_ii\n",
    "        n_io = n_xi - n_ii\n",
    "        n_oi = n_ix - n_ii\n",
    "        n_oo = n_xx - n_ii - n_oi - n_io\n",
    "        return n_xx * (float((n_ii*n_oo - n_io*n_oi)**2) /\n",
    "                       ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))\n",
    "\n",
    "    def best_words(self, num, need_score=False):\n",
    "        words = sorted(self.words.items(), key=lambda word_pair: word_pair[1], reverse=True)\n",
    "        if need_score:\n",
    "            return [word for word in words[:num]]\n",
    "        else:\n",
    "            return [word[0] for word in words[:num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "fe = ChiSquare(train_texts, train_labels)\n",
    "best_words = fe.best_words(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bow_feature = text_to_vector(best_words, x_train) # 将best_words换成bog会爆内存\n",
    "print(train_bow_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_bow_feature = text_to_vector(best_words, x_valid)\n",
    "test_bow_feature = text_to_vector(best_words, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_bow_feature, y_train=shuffle(train_bow_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用逻辑回归算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=2019, \n",
    "                         solver='sag', #优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                         multi_class='multinomial' #分类方式：multinomial、ovr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=2019, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(train_bow_feature, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5963091118800461\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(valid_bow_feature)\n",
    "print(np.mean(predict == y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict(test_bow_feature)\n",
    "submission = pd.read_csv('./sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n",
    "submission['Sentiment'] = res\n",
    "submission.to_csv('./submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用词频TF特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 15230) (31212, 15230)\n",
      "0.6563821607074202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 提取文本计数特征 -- 每个单词的数量\n",
    "# 对文本的单词进行计数，包括文本的预处理, 分词以及过滤停用词\n",
    "train_texts = list(train['Phrase'].values)\n",
    "train_labels = train['Sentiment'].values\n",
    "test_texts = list(test['Phrase'].values)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_valid_counts = count_vect.transform(x_valid)\n",
    "print(x_train_counts.shape, x_valid_counts.shape)  # (93636, 15188) (31212, 15188)  矩阵(句子-词汇）的维度，词表大小15188\n",
    "# 在词汇表中一个单词的索引值对应的是该单词在整个训练的文集中出现的频率。\n",
    "# print(count_vect.vocabulary_.get(u'good'))    #5812     count_vect.vocabulary_是一个词典：word-id\n",
    "x_train_counts, y_train=shuffle(x_train_counts, y_train)\n",
    "clf = LogisticRegression(random_state=2019, solver='saga',  # 优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                             multi_class='multinomial',  # 分类方式：multinomial、ovr\n",
    "                             max_iter=1000).fit(x_train_counts, y_train)\n",
    "predict = clf.predict(x_valid_counts)\n",
    "print(np.mean(predict == y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用词频TF-IDF特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 15232) (31212, 15232)\n",
      "0.6346917852108164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 提取TF-IDF特征 -- word级别的TF-IDF\n",
    "# 将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频tf。\n",
    "train_texts = list(train['Phrase'].values)\n",
    "train_labels = train['Sentiment'].values\n",
    "test_texts = list(test['Phrase'].values)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_word = tfidf_transformer.transform(x_train)\n",
    "x_valid_tfidf_word = tfidf_transformer.transform(x_valid)\n",
    "print(x_train_tfidf_word.shape, x_valid_tfidf_word.shape)\n",
    "x_train_tfidf_word, y_train=shuffle(x_train_tfidf_word, y_train)\n",
    "clf = LogisticRegression(random_state=2019, solver='saga',  # 优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                             multi_class='multinomial',  # 分类方式：multinomial、ovr\n",
    "                             max_iter=1000).fit(x_train_tfidf_word, y_train)\n",
    "predict = clf.predict(x_valid_tfidf_word)\n",
    "print(np.mean(predict == y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用词频3-gram TF-IDF特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 50000) (31212, 50000)\n",
      "0.5996091246956299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 提取TF-IDF特征 - ngram级别的TF-IDF\n",
    "# 将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频tf。\n",
    "train_texts = list(train['Phrase'].values)\n",
    "train_labels = train['Sentiment'].values\n",
    "test_texts = list(test['Phrase'].values)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_ngram = tfidf_transformer.transform(x_train)\n",
    "x_valid_tfidf_ngram = tfidf_transformer.transform(x_valid)\n",
    "print(x_train_tfidf_ngram.shape, x_valid_tfidf_ngram.shape)\n",
    "x_train_tfidf_ngram, y_train=shuffle(x_train_tfidf_ngram, y_train)\n",
    "clf = LogisticRegression(random_state=2019, solver='saga',  # 优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                             multi_class='multinomial',  # 分类方式：multinomial、ovr\n",
    "                             max_iter=1000).fit(x_train_tfidf_ngram, y_train)\n",
    "predict = clf.predict(x_valid_tfidf_ngram)\n",
    "print(np.mean(predict == y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用组合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6640715109573241\n"
     ]
    }
   ],
   "source": [
    "%% time\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "train_texts = list(train['Phrase'].values)\n",
    "train_labels = train['Sentiment'].values\n",
    "test_texts = list(test['Phrase'].values)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "train_bow_feature = text_to_vector(best_words, x_train)\n",
    "valid_bow_feature = text_to_vector(best_words, x_valid)\n",
    "test_bow_feature = text_to_vector(best_words, test_texts)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_valid_counts = count_vect.transform(x_valid)\n",
    "x_test_counts = count_vect.transform(test_texts)\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_word = tfidf_transformer.transform(x_train)\n",
    "x_valid_tfidf_word = tfidf_transformer.transform(x_valid)\n",
    "x_test_tfidf_word = tfidf_transformer.transform(test_texts)\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), max_features=50000)\n",
    "tfidf_transformer.fit(x_train)\n",
    "x_train_tfidf_ngram = tfidf_transformer.transform(x_train)\n",
    "x_valid_tfidf_ngram = tfidf_transformer.transform(x_valid)\n",
    "x_test_tfidf_ngram = tfidf_transformer.transform(test_texts)\n",
    "\n",
    "train_features = hstack([np.array(train_bow_feature), x_train_counts, x_train_tfidf_word, x_train_tfidf_ngram])\n",
    "valid_features = hstack([np.array(valid_bow_feature), x_valid_counts, x_valid_tfidf_word, x_valid_tfidf_ngram]) \n",
    "test_features = hstack([np.array(test_bow_feature), x_test_counts, x_test_tfidf_word, x_test_tfidf_ngram])\n",
    "\n",
    "\n",
    "x_train_tfidf_ngram, y_train=shuffle(train_features, y_train)\n",
    "clf = LogisticRegression(random_state=2019, solver='saga',  # 优化算法：liblinear、lbfgs、newton-cg、sag\n",
    "                             multi_class='multinomial',  # 分类方式：multinomial、ovr\n",
    "                             max_iter=1000).fit(x_train_tfidf_ngram, y_train)\n",
    "predict = clf.predict(valid_features)\n",
    "print(np.mean(predict == y_valid))\n",
    "\n",
    "reslts = clf.predict(test_features)\n",
    "submission = pd.read_csv('./sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n",
    "submission['Sentiment'] = reslts\n",
    "submission.to_csv('./all_features_submission.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "### 1、数据清洗\n",
    "### 2、尝试其他分类器\n",
    "### 3、利用matplotlib进行数据探索性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_word_frequncy(all_sentences):\n",
    "#     bag_of_words = {}\n",
    "#     for sentence in all_sentences:\n",
    "#         for word in sentence.split():\n",
    "#             if word in bag_of_words:\n",
    "#                 bag_of_words[word] += 1\n",
    "#             else:\n",
    "#                 bag_of_words[word] = 1\n",
    "#     return bag_of_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
