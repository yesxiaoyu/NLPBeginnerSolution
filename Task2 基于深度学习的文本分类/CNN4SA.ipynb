{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务二：基于深度学习的文本分类\n",
    "熟悉Pytorch，用Pytorch重写《任务一》，实现CNN、RNN的文本分类；\n",
    "\n",
    "## 1、参考\n",
    "\n",
    "    ·https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html?highlight=text%20classification\n",
    "    ·Convolutional Neural Networks for Sentence Classification https://arxiv.org/abs/1408.5882\n",
    "    ·https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "## 2、word embedding 的方式初始化\n",
    "\n",
    "    ·随机embedding的初始化方式\n",
    "\n",
    "    ·用glove 预训练的embedding进行初始化 https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "## 3、知识点：\n",
    "\n",
    "    ·CNN/RNN的特征抽取\n",
    "    ·词嵌入\n",
    "    ·Dropout\n",
    "    \n",
    "## 4、时间：两周\n",
    "\n",
    "-----------------------------\n",
    "## 参考：\n",
    "### 官方文档：https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "### pytorch 情感分析，无TorchText https://blog.csdn.net/captain_f_/article/details/89331133\n",
    "### [TorchText]使用 https://www.jianshu.com/p/e5adb235399e\n",
    "### pytorch 情感分析 https://blog.csdn.net/weixin_34351321/article/details/94699262\n",
    "### 大佬的NLPBeginner解答 https://github.com/htfhxx/nlp-beginner_solution\n",
    "\n",
    "## TODO:\n",
    "### 1、EDA可视化\n",
    "### 2、matplotlib画出模型收敛曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "import numpy as np, pandas as pd\n",
    "from torchtext import data,datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size=32\n",
    "embedding_dim =300\n",
    "dropout_p=0.5\n",
    "filters_num=100\n",
    "use_cuda = 1\n",
    "learning_rate = 0.001\n",
    "epochs=5\n",
    "seed = 2019\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./sentiment-analysis-on-movie-reviews/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('./sentiment-analysis-on-movie-reviews/test.tsv', sep='\\t')\n",
    "\n",
    "#shuffle、划分验证集、测试集,并保存\n",
    "idx =np.arange(train.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_size=int(len(idx) * 0.8)\n",
    "\n",
    "train.iloc[idx[:train_size], :].to_csv('./train.csv',index=None)\n",
    "train.iloc[idx[train_size:], :].to_csv('./valid.csv', index=None)\n",
    "test.to_csv('./test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "# tokenizer = lambda x: x.split()\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "\n",
    "train_data = data.TabularDataset(path='./train.csv',format='csv', skip_header=True,\n",
    "        fields = [('PhraseId', None),('SentenceId', None),('Phrase', TEXT),('Sentiment', LABEL)])\n",
    "valid_data = data.TabularDataset(path='./valid.csv',format='csv', skip_header=True,\n",
    "        fields = [('PhraseId', None),('SentenceId', None),('Phrase', TEXT),('Sentiment', LABEL)])\n",
    "test_data = data.TabularDataset(path='./test.csv',format='csv', skip_header=True,\n",
    "        fields = [('PhraseId', None),('SentenceId', None),('Phrase', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词典，字符映射到embedding\n",
    "#TEXT.vocab.vectors 就是词向量\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建迭代器\n",
    "train_iterator = data.BucketIterator(train_data, batch_size=batch_size, train=True, shuffle=True, device=device)\n",
    "\n",
    "valid_iterator = data.Iterator(valid_data, batch_size=batch_size, train=False, sort=False, device=device)\n",
    "\n",
    "test_iterator = data.Iterator(test_data, batch_size=batch_size, train=False, sort=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15427 6\n"
     ]
    }
   ],
   "source": [
    "#部分参数设置\n",
    "embedding_choice= 'rand'  #  'glove'    'static'    'non-static'\n",
    "num_embeddings = len(TEXT.vocab)\n",
    "\n",
    "vocab_size=len(TEXT.vocab)\n",
    "label_num=len(LABEL.vocab)\n",
    "print(vocab_size,label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embedding_choice=embedding_choice\n",
    "        \n",
    "        if self.embedding_choice==  'rand':\n",
    "            self.embedding=nn.Embedding(num_embeddings,embedding_dim)\n",
    "        if self.embedding_choice==  'glove':\n",
    "            self.embedding = nn.Embedding(num_embeddings, embedding_dim, \n",
    "                padding_idx=PAD_INDEX).from_pretrained(TEXT.vocab.vectors, freeze=True)\n",
    "            \n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(3, embedding_dim), padding=(2,0))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(4, embedding_dim), padding=(3,0))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=1,out_channels=filters_num ,  #卷积产生的通道\n",
    "                               kernel_size=(5, embedding_dim), padding=(4,0))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(filters_num * 3, label_num)\n",
    "        \n",
    "    def forward(self,x):      # (Batch_size, Length) \n",
    "        x=self.embedding(x).unsqueeze(1)      #(Batch_size, Length, Dimention) \n",
    "                                       #(Batch_size, 1, Length, Dimention) \n",
    "        \n",
    "        x1 = F.relu(self.conv1(x)).squeeze(3)    #(Batch_size, filters_num, length+padding, 1) \n",
    "                                          #(Batch_size, filters_num, length+padding) \n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)  #(Batch_size, filters_num, 1)\n",
    "                                               #(Batch_size, filters_num) \n",
    "         \n",
    "        x2 = F.relu(self.conv2(x)).squeeze(3)  \n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)      \n",
    "        \n",
    "        x3 = F.relu(self.conv3(x)).squeeze(3)  \n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)      \n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), dim=1)  #(Batch_size, filters_num *3 )\n",
    "        x = self.dropout(x)      #(Batch_size, filters_num *3 )\n",
    "        out = self.fc(x)       #(Batch_size, label_num  )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建模型\n",
    "\n",
    "model=CNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#创建优化器SGD\n",
    "criterion = nn.CrossEntropyLoss()   #损失函数\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy=0\n",
    "start_time=time.time()\n",
    "\n",
    "def train(model, epoch):\n",
    "\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(train_iterator.dataset)\n",
    "    steps = 0.0\n",
    "    for batch in tqdm(train_iterator):\n",
    "        steps += 1\n",
    "        optimizer.zero_grad()\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch.Phrase)#[batch_size, label_num]\n",
    "        \n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1]  #get the indices\n",
    "                   .view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "\n",
    "    print(\"Epoch %d_%.3f%%:  Training average Loss: %f, Total Time:%f\"\n",
    "    %(epoch, steps * train_iterator.batch_size*100/len(train_iterator.dataset),total_loss/steps, time.time()-start_time))\n",
    "\n",
    "def valid(model, epoch):\n",
    "    #每个epoch都验证一下\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    accuracy=0.0\n",
    "    total_correct=0.0\n",
    "    total_data_num = len(valid_iterator.dataset)\n",
    "    steps = 0.0 \n",
    "    for batch in tqdm(valid_iterator):\n",
    "        steps+=1\n",
    "        batch_label = batch.Sentiment\n",
    "        out = model(batch.Phrase)\n",
    "        loss = criterion(out, batch_label)\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        correct = (torch.max(out, dim=1)[1].view(batch_label.size()) == batch_label).sum()\n",
    "        total_correct = total_correct + correct.item()\n",
    "\n",
    "    print(\"Epoch %d :  Verification average Loss: %f, Verification accuracy: %f%%,Total Time:%f\"\n",
    "      %(epoch, total_loss/steps, total_correct*100/total_data_num,time.time()-start_time)) \n",
    "    global best_accuracy\n",
    "    if best_accuracy < total_correct/total_data_num :\n",
    "        best_accuracy = total_correct/total_data_num \n",
    "#         torch.save(model,'./epoch_%d_accuracy_%f'%(epoch,total_correct/total_data_num))\n",
    "#         print('Model is saved in ./epoch_%d_accuracy_%f'%(epoch,total_correct/total_data_num))\n",
    "        torch.save(model,'./cnn_best_model.pt')\n",
    "        print('Model is saved in ./cnn_best_model.pt %d_accuracy_%f'%(epoch,total_correct/total_data_num))\n",
    "    \n",
    "def test(model):\n",
    "    result = torch.LongTensor().cuda()\n",
    "    start_time=time.time()\n",
    "    for batch in tqdm(test_iterator):\n",
    "        result = torch.cat((result, torch.max(model(batch.Phrase), dim=1)[1]), 0)\n",
    "    print('Total time: %f',time.time()-start_time)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:37<00:00, 102.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0_100.013%:  Training average Loss: 1.112462, Total Time:37.947817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:02<00:00, 423.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :  Verification average Loss: 0.936803, Verification accuracy: 61.485967%,Total Time:40.255161\n",
      "Model is saved in ./best_model.pt 0_accuracy_0.614860\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:36<00:00, 107.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1_100.013%:  Training average Loss: 0.921917, Total Time:76.504708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:02<00:00, 426.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 :  Verification average Loss: 0.892346, Verification accuracy: 64.414328%,Total Time:78.799110\n",
      "Model is saved in ./best_model.pt 1_accuracy_0.644143\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:36<00:00, 107.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2_100.013%:  Training average Loss: 0.847097, Total Time:115.025298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:02<00:00, 424.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 :  Verification average Loss: 0.871296, Verification accuracy: 64.452775%,Total Time:117.331182\n",
      "Model is saved in ./best_model.pt 2_accuracy_0.644528\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:36<00:00, 108.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3_100.013%:  Training average Loss: 0.802887, Total Time:153.458855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:02<00:00, 428.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 :  Verification average Loss: 0.869656, Verification accuracy: 65.378700%,Total Time:155.740357\n",
      "Model is saved in ./best_model.pt 3_accuracy_0.653787\n",
      "============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:36<00:00, 107.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4_100.013%:  Training average Loss: 0.770498, Total Time:191.923795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [00:02<00:00, 431.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 :  Verification average Loss: 0.867726, Verification accuracy: 65.981033%,Total Time:194.186745\n",
      "Model is saved in ./best_model.pt 4_accuracy_0.659810\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, epoch)\n",
    "    valid(model, epoch)\n",
    "    print('============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2072/2072 [00:03<00:00, 539.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: %f 3.8445041179656982\n"
     ]
    }
   ],
   "source": [
    "# # 加载最好的模型\n",
    "best_model = torch.load('./cnn_best_model.pt')\n",
    "result = test(best_model)\n",
    "submission = pd.read_csv('./sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\n",
    "submission['Sentiment'] = result.cpu()\n",
    "submission.to_csv('./CNN_submission.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
